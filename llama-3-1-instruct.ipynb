{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9685718,"sourceType":"datasetVersion","datasetId":5920860}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install -q transformers torch sentencepiece bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-10-21T21:39:19.186592Z","iopub.execute_input":"2024-10-21T21:39:19.187360Z","iopub.status.idle":"2024-10-21T21:39:37.245568Z","shell.execute_reply.started":"2024-10-21T21:39:19.187316Z","shell.execute_reply":"2024-10-21T21:39:37.244546Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"! pip install -U bitsandbytes\n! pip install -U accelerate","metadata":{"execution":{"iopub.status.busy":"2024-10-21T21:39:42.346786Z","iopub.execute_input":"2024-10-21T21:39:42.347583Z","iopub.status.idle":"2024-10-21T21:40:06.996708Z","shell.execute_reply.started":"2024-10-21T21:39:42.347542Z","shell.execute_reply":"2024-10-21T21:40:06.995689Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting accelerate\n  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.34.2\n    Uninstalling accelerate-0.34.2:\n      Successfully uninstalled accelerate-0.34.2\nSuccessfully installed accelerate-1.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=\"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\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T21:40:21.886665Z","iopub.execute_input":"2024-10-21T21:40:21.887044Z","iopub.status.idle":"2024-10-21T21:40:22.539831Z","shell.execute_reply.started":"2024-10-21T21:40:21.887007Z","shell.execute_reply":"2024-10-21T21:40:22.538918Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    pipeline,\n    logging,\n)\nfrom transformers.generation.configuration_utils import GenerationConfig\n\n# Define the model name\nmodel_name = \"meta-llama/Llama-3.1-8b-instruct\"  # Update the model name if necessary\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n)\n\nmodel.gradient_checkpointing_enable()\nmodel.config.use_cache = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-10-21T21:40:26.155231Z","iopub.execute_input":"2024-10-21T21:40:26.155959Z","iopub.status.idle":"2024-10-21T21:43:26.582646Z","shell.execute_reply.started":"2024-10-21T21:40:26.155917Z","shell.execute_reply":"2024-10-21T21:43:26.581827Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"054c5578e256428491232bdb3b916a96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b37cf14de69244579f7cedcfa3b2d49a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e888ce2affc4451a4125b1f9779ef17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f1e46468c10418b9cdbf3d824927651"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b382c5a2aeb4f41ad04c077cb5d4b2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca5da9ae7950439786c93d06f9e35191"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9cf130303db4978a7e6893d490bffc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24ec5f4839334961a9360fe4edc7c77a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6d963a6180247caabfde3f1726422c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72a24f29f49546aabfc9995a6446a953"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ec5ba6d3f00422285e8fe44fce6c51b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4004e184636485d840259eb801fc7e6"}},"metadata":{}}]},{"cell_type":"code","source":"import csv\nimport random\n\n# Path to the CSV file\ndata_file_path = \"/kaggle/input/paragraph-ground-truth/ground_truth.csv\"\nwith open(data_file_path, 'r', encoding='utf-8') as csvfile:\n    reader = csv.DictReader(csvfile)\n    data = list(reader)\n\n# Function to prepare few-shot examples from CSV\ndef prepare_few_shot_examples(num_examples):\n    examples = []\n    # Randomly select num_examples rows\n    selected_rows = random.sample(data, num_examples)\n\n    for row in selected_rows:\n      original_text = row['original']\n      annotated_text = row['cleaned']\n\n      # Format for few-shot prompting\n      examples.append({\n          \"original\": original_text,\n          \"segmented\": annotated_text\n      })\n\n    return examples\n\n# Prepare a few examples (you can adjust the number)\nfew_shot_examples = prepare_few_shot_examples(num_examples=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T21:45:26.124903Z","iopub.execute_input":"2024-10-21T21:45:26.126011Z","iopub.status.idle":"2024-10-21T21:45:26.217155Z","shell.execute_reply.started":"2024-10-21T21:45:26.125968Z","shell.execute_reply":"2024-10-21T21:45:26.216136Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# def prepare_few_shot_prompt(few_shot_examples, test_document):\n#     prompt = \"\"\"\n#     You are an intelligent AI model which when given a legal document, chunk the whole document into paragraphs.\n#     You shall be given an example with the side heading 'Example:' which is a chunked document according to the format mentioned above,\n#     learn how it is being done.\n#     Ensure that each paragraph focuses on a distinct point, such as case information, legal arguments, judgments, or procedural steps. \n#     Format: Use '<para>'to indicate the beginning of a paragraph, and '</para>' to indicate the end of that paragraph.\n#     Do not summarize or condense the text, just chunk it into paragraph as is using the above format.\n#     You shall be given a legal document with the side heading 'Test:' that you have to break into meaningful paragraphs based on the learning.\n#     Don't produce any junk, just output the chunked document following the format mentioned above.\n#     \"\"\"\n\n#     # Add few-shot examples to the prompt\n#     for example in few_shot_examples:\n#         prompt += f\"\"\"Example: {example['segmented']}.\"\"\"\n\n# #     Add the test document as the new example for inference\n#     prompt += f\"\"\"\n#     here's the document you need to chunk.\n#     Test: {test_document}\"\"\"\n\n#     return prompt","metadata":{"execution":{"iopub.status.busy":"2024-10-21T18:45:07.297670Z","iopub.execute_input":"2024-10-21T18:45:07.298507Z","iopub.status.idle":"2024-10-21T18:45:07.307020Z","shell.execute_reply.started":"2024-10-21T18:45:07.298449Z","shell.execute_reply":"2024-10-21T18:45:07.305776Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def prepare_few_shot_prompt(few_shot_examples, test_document):\n    prompt = \"\"\"\n    You are an intelligent AI model which when given a legal document, chunk the whole document into paragraphs.\n    Ensure that each paragraph focuses on a distinct point, such as case information, legal arguments, judgments, or procedural steps. \n    Format: Use '<para>'to indicate the beginning of a paragraph, and '</para>' to indicate the end of that paragraph.\n    Do not summarize or condense the text, just chunk it into paragraph as is using the above format.\n    You shall be given a legal document with the side heading 'Test:' that you have to break into meaningful paragraphs based on the learning.\n    Don't produce any junk, just output the chunked document following the format mentioned above.\n    \"\"\"\n\n#     # Add few-shot examples to the prompt\n#     for example in few_shot_examples:\n#         prompt += f\"\"\"Example: {example['segmented']}.\"\"\"\n\n#     Add the test document as the new example for inference\n    prompt += f\"\"\"\n    here's the document you need to chunk.\n    Test: {test_document}\"\"\"\n\n    return prompt","metadata":{"execution":{"iopub.status.busy":"2024-10-21T21:46:09.490786Z","iopub.execute_input":"2024-10-21T21:46:09.491174Z","iopub.status.idle":"2024-10-21T21:46:09.496820Z","shell.execute_reply.started":"2024-10-21T21:46:09.491136Z","shell.execute_reply":"2024-10-21T21:46:09.495860Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Function to perform inference\ndef infer_paragraph_segmentation(model, tokenizer, prompt, max_length=35000):\n    # Tokenize input and move to the specified device\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).input_ids.to('cuda', non_blocking=True)\n    \n    torch.cuda.empty_cache()\n\n    # Generate output from the model\n    with torch.no_grad():\n        outputs = model.generate(inputs, max_length=max_length, temperature=0.7, do_sample=True)\n\n    # Decode the generated output\n    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return output_text","metadata":{"execution":{"iopub.status.busy":"2024-10-21T21:46:21.743020Z","iopub.execute_input":"2024-10-21T21:46:21.743743Z","iopub.status.idle":"2024-10-21T21:46:21.749922Z","shell.execute_reply.started":"2024-10-21T21:46:21.743701Z","shell.execute_reply":"2024-10-21T21:46:21.748980Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"test_row = random.sample(data, 1)\nfor row in test_row:\n    test_doc = row['original']\n\n# Prepare the few-shot prompt\nfew_shot_prompt = prepare_few_shot_prompt(few_shot_examples, test_doc)\n\n# Perform inference\nsegmented_output = infer_paragraph_segmentation(model, tokenizer, few_shot_prompt)\n\n# Extract the segmented output (removing the few-shot prompt part)\n# segmented_output = segmented_output.split(\"Segmented:\")[1].strip()\n\n# Print the final segmented output\nprint(\"Segmented Document:\")\nprint(segmented_output)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T21:46:33.901801Z","iopub.execute_input":"2024-10-21T21:46:33.902177Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}