{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9681670,"sourceType":"datasetVersion","datasetId":5917884}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\n\n# Set paths for original and annotated documents\noriginal_docs_path = \"/kaggle/input/para-splitting-data/data_splitting/original\"\nannotated_docs_path = \"/kaggle/input/para-splitting-data/data_splitting/annotated\"\n\n# Load the 50 documents with the naming convention in mind\ndef load_documents_with_naming_convention(original_docs_path, annotated_docs_path):\n    original_files = sorted(os.listdir(original_docs_path))\n    original_documents = []\n    annotated_documents = []\n    \n    for file in original_files:\n        if file.endswith('.txt'):\n            # Load the original document\n            with open(os.path.join(original_docs_path, file), 'r', encoding='utf-8') as f:\n                original_documents.append(f.read())\n            \n            # Load the corresponding annotated document\n            annotated_file = 'cleaned_' + file\n            with open(os.path.join(annotated_docs_path, annotated_file), 'r', encoding='utf-8') as f:\n                annotated_documents.append(f.read())\n    \n    return original_documents, annotated_documents\n\n# Load original and corresponding annotated documents\noriginal_documents, annotated_documents = load_documents_with_naming_convention(original_docs_path, annotated_docs_path)\n\nprint(f\"Loaded {len(original_documents)} original and {len(annotated_documents)} annotated documents.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-21T09:33:52.706099Z","iopub.execute_input":"2024-10-21T09:33:52.706958Z","iopub.status.idle":"2024-10-21T09:33:53.134711Z","shell.execute_reply.started":"2024-10-21T09:33:52.706919Z","shell.execute_reply":"2024-10-21T09:33:53.133822Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Loaded 50 original and 50 annotated documents.\n","output_type":"stream"}]},{"cell_type":"code","source":"class ParagraphT5Dataset(Dataset):\n    def __init__(self, original_docs, annotated_docs, tokenizer, max_len=512):\n        self.original_docs = original_docs\n        self.annotated_docs = annotated_docs\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.original_docs)\n\n    def __getitem__(self, idx):\n        original_text = self.original_docs[idx]\n        annotated_text = self.annotated_docs[idx]\n        \n        # Tokenize input and output\n        input_encoding = self.tokenizer(original_text, max_length=self.max_len, truncation=True, padding='max_length', return_tensors=\"pt\")\n        target_encoding = self.tokenizer(annotated_text, max_length=self.max_len, truncation=True, padding='max_length', return_tensors=\"pt\")\n        \n        input_ids = input_encoding['input_ids'].squeeze()  # Remove batch dimension\n        attention_mask = input_encoding['attention_mask'].squeeze()\n        labels = target_encoding['input_ids'].squeeze()\n\n        return input_ids, attention_mask, labels","metadata":{"execution":{"iopub.status.busy":"2024-10-21T09:34:18.342913Z","iopub.execute_input":"2024-10-21T09:34:18.343766Z","iopub.status.idle":"2024-10-21T09:34:18.351269Z","shell.execute_reply.started":"2024-10-21T09:34:18.343725Z","shell.execute_reply":"2024-10-21T09:34:18.350383Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained T5 tokenizer\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\n\n# Create the dataset\ndataset = ParagraphT5Dataset(original_documents, annotated_documents, tokenizer)\n\n# Split into training and validation sets\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=2)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T09:34:38.101190Z","iopub.execute_input":"2024-10-21T09:34:38.102047Z","iopub.status.idle":"2024-10-21T09:34:38.650624Z","shell.execute_reply.started":"2024-10-21T09:34:38.101992Z","shell.execute_reply":"2024-10-21T09:34:38.649454Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained('t5-base')\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Optimizer and learning rate\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T09:34:50.878419Z","iopub.execute_input":"2024-10-21T09:34:50.879057Z","iopub.status.idle":"2024-10-21T09:34:52.796674Z","shell.execute_reply.started":"2024-10-21T09:34:50.879001Z","shell.execute_reply":"2024-10-21T09:34:52.795719Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def train(model, dataloader, optimizer, device):\n    model.train()\n    total_loss = 0\n    for batch in dataloader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        \n        # Clear the gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(dataloader)\n    return avg_loss","metadata":{"execution":{"iopub.status.busy":"2024-10-21T09:35:05.963978Z","iopub.execute_input":"2024-10-21T09:35:05.964983Z","iopub.status.idle":"2024-10-21T09:35:05.971226Z","shell.execute_reply.started":"2024-10-21T09:35:05.964944Z","shell.execute_reply":"2024-10-21T09:35:05.970206Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, dataloader, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n    \n    avg_loss = total_loss / len(dataloader)\n    return avg_loss","metadata":{"execution":{"iopub.status.busy":"2024-10-21T09:35:16.300056Z","iopub.execute_input":"2024-10-21T09:35:16.300929Z","iopub.status.idle":"2024-10-21T09:35:16.306683Z","shell.execute_reply.started":"2024-10-21T09:35:16.300888Z","shell.execute_reply":"2024-10-21T09:35:16.305706Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Training process\nepochs = 3\nfor epoch in range(epochs):\n    train_loss = train(model, train_loader, optimizer, device)\n    val_loss = evaluate(model, val_loader, device)\n    print(f\"Epoch {epoch+1}/{epochs}:\")\n    print(f\"Training Loss: {train_loss:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T09:36:25.637291Z","iopub.execute_input":"2024-10-21T09:36:25.638254Z","iopub.status.idle":"2024-10-21T09:37:14.377079Z","shell.execute_reply.started":"2024-10-21T09:36:25.638213Z","shell.execute_reply":"2024-10-21T09:37:14.376128Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch 1/3:\nTraining Loss: 1.8890\nValidation Loss: 0.7025\nEpoch 2/3:\nTraining Loss: 1.0721\nValidation Loss: 0.3939\nEpoch 3/3:\nTraining Loss: 0.7206\nValidation Loss: 0.2676\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the fine-tuned model\nmodel.save_pretrained('t5_paragraph_splitter')\ntokenizer.save_pretrained('t5_paragraph_splitter')","metadata":{"execution":{"iopub.status.busy":"2024-10-21T09:37:23.290395Z","iopub.execute_input":"2024-10-21T09:37:23.291141Z","iopub.status.idle":"2024-10-21T09:37:25.350051Z","shell.execute_reply.started":"2024-10-21T09:37:23.291099Z","shell.execute_reply":"2024-10-21T09:37:25.349109Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('t5_paragraph_splitter/tokenizer_config.json',\n 't5_paragraph_splitter/special_tokens_map.json',\n 't5_paragraph_splitter/spiece.model',\n 't5_paragraph_splitter/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"# Function to load a document from a .txt file\ndef load_test_document(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        document = file.read()\n    return document\n\n# Updated function to split paragraphs using a loaded .txt file\ndef split_paragraphs_from_file(model, tokenizer, file_path, device):\n    # Load the document from the file\n    document = load_test_document(file_path)\n    \n    # Tokenize and generate the output\n    input_ids = tokenizer(document, return_tensors='pt', truncation=True, padding=True).input_ids.to(device)\n    \n    # Generate the segmented document\n    with torch.no_grad():\n        outputs = model.generate(input_ids=input_ids, max_length=512)\n    \n    # Decode the output\n    segmented_document = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return segmented_document","metadata":{"execution":{"iopub.status.busy":"2024-10-21T09:40:33.310553Z","iopub.execute_input":"2024-10-21T09:40:33.310961Z","iopub.status.idle":"2024-10-21T09:40:33.318428Z","shell.execute_reply.started":"2024-10-21T09:40:33.310922Z","shell.execute_reply":"2024-10-21T09:40:33.317416Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Specify the path to the test .txt file\ntest_file_path = \"/kaggle/input/para-splitting-data/data_splitting/original/_judgment_judis_29890.txt\"  # Replace with the actual file path\n\n# Perform inference and print the segmented output\nsegmented_output = split_paragraphs_from_file(model, tokenizer, test_file_path, device)\nprint(\"Segmented Document with Paragraphs:\")\nprint(segmented_output)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T09:40:57.108162Z","iopub.execute_input":"2024-10-21T09:40:57.108561Z","iopub.status.idle":"2024-10-21T09:41:06.143855Z","shell.execute_reply.started":"2024-10-21T09:40:57.108525Z","shell.execute_reply":"2024-10-21T09:41:06.142908Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"Segmented Document with Paragraphs:\nthe appeal has been preferred against the judgment and order dated 2.1.2007 of Gauhati High Court by which the appeal preferred by the appellants was disposed of with the modification that the sentence of five years R.I. and fine of Rs.7,000/- imposed upon each of the appellants under Section 313 read with Section 34 IPC by the learned Additional Sessions Judge, Kokrajhar, was reduced to three years R.I. and fine of Rs.5,000/- of both the appellants. The court took regard to the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact that the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the fact of the\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}